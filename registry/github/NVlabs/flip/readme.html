<div id="readme" class="md" data-path="README.md"><article class="markdown-body entry-content container-lg" itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer" href="images/teaser.png"><img src="images/teaser.png" alt="Teaser image" title="Teaser image" style="max-width: 100%;"></a></p>
<div class="markdown-heading" dir="auto"><h1 class="heading-element" dir="auto">ꟻLIP: A Tool for Visualizing and Communicating Errors in Rendered Images (v1.3)</h1><a id="user-content-ꟻlip-a-tool-for-visualizing-and-communicating-errors-in-rendered-images-v13" class="anchor" aria-label="Permalink: ꟻLIP: A Tool for Visualizing and Communicating Errors in Rendered Images (v1.3)" href="#ꟻlip-a-tool-for-visualizing-and-communicating-errors-in-rendered-images-v13"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">By
<a href="https://research.nvidia.com/person/pontus-ebelin" rel="nofollow">Pontus Ebelin</a>,
Jim Nilsson,
and
<a href="https://research.nvidia.com/person/tomas-akenine-m%C3%B6ller" rel="nofollow">Tomas Akenine-Möller</a>,
with
<a href="https://www1.maths.lth.se/matematiklth/personal/magnuso/" rel="nofollow">Magnus Oskarsson</a>,
<a href="https://www.maths.lu.se/staff/kalleastrom/" rel="nofollow">Kalle Åström</a>,
<a href="https://www.rit.edu/directory/mdfpph-mark-fairchild" rel="nofollow">Mark D. Fairchild</a>,
and
<a href="https://research.nvidia.com/person/peter-shirley" rel="nofollow">Peter Shirley</a>.</p>
<p dir="auto">This repository holds implementations of the <a href="https://research.nvidia.com/publication/2020-07_FLIP" rel="nofollow">LDR-ꟻLIP</a>
and <a href="https://research.nvidia.com/publication/2021-05_HDR-FLIP" rel="nofollow">HDR-ꟻLIP</a> image error metrics.
It also holds code for the ꟻLIP tool, presented in <a href="https://www.realtimerendering.com/raytracinggems/rtg2/index.html" rel="nofollow">Ray Tracing Gems II</a>.</p>
<p dir="auto">The changes made for the different versions of ꟻLIP are summarized in the <a href="misc/versionList.md">version list</a>.</p>
<p dir="auto"><a href="misc/papersUsingFLIP.md">A list of papers</a> that use/cite ꟻLIP.</p>
<p dir="auto"><a href="misc/precision.md">A note</a> about the precision of ꟻLIP.</p>
<p dir="auto"><a href="https://research.nvidia.com/node/3525" rel="nofollow">An image gallery</a> displaying a large quantity of reference/test images and corresponding error maps from
different metrics.</p>
<p dir="auto"><strong>Note</strong>: in v1.3, we switched to a <em>single header</em> (<a href="https://github.com/NVlabs/flip/blob/singleheader_WIP/cpp/FLIP.h">FLIP.h</a>) for C++/CUDA for easier integration.</p>
<div class="markdown-heading" dir="auto"><h1 class="heading-element" dir="auto">License</h1><a id="user-content-license" class="anchor" aria-label="Permalink: License" href="#license"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Copyright © 2020-2024, NVIDIA Corporation &amp; Affiliates. All rights reserved.</p>
<p dir="auto">This work is made available under a <a href="misc/LICENSE.md">BSD 3-Clause License</a>.</p>
<p dir="auto">The repository distributes code for <code>tinyexr</code>, which is subject to a <a href="misc/LICENSE-third-party.md#bsd-3-clause-license">BSD 3-Clause License</a>,<br>
and <code>stb_image</code>, which is subject to an <a href="misc/LICENSE-third-party.md#mit-license">MIT License</a>.</p>
<p dir="auto">For individual contributions to the project, please confer the <a href="misc/CLA.md">Individual Contributor License Agreement</a>.</p>
<p dir="auto">For business inquiries, please visit our website and submit the form: <a href="https://www.nvidia.com/en-us/research/inquiries/" rel="nofollow">NVIDIA Research Licensing</a>.</p>
<div class="markdown-heading" dir="auto"><h1 class="heading-element" dir="auto">Python (API and Tool)</h1><a id="user-content-python-api-and-tool" class="anchor" aria-label="Permalink: Python (API and Tool)" href="#python-api-and-tool"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><strong>Setup</strong> (with Anaconda3):</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="conda create -n flip python numpy matplotlib
conda activate flip
conda install -c conda-forge opencv
conda install -c conda-forge openexr-python"><pre class="notranslate"><code>conda create -n flip python numpy matplotlib
conda activate flip
conda install -c conda-forge opencv
conda install -c conda-forge openexr-python
</code></pre></div>
<p dir="auto"><strong>Usage:</strong></p>
<p dir="auto"><em>Remember to activate the</em> <code>flip</code> <em>environment through</em> <code>conda activate flip</code> <em>before using the tool.</em></p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="python flip.py --reference reference.{exr|png} --test test.{exr|png} [--options]"><pre class="notranslate"><code>python flip.py --reference reference.{exr|png} --test test.{exr|png} [--options]
</code></pre></div>
<p dir="auto">See the <a href="python/README.md">README</a> in the <code>python</code> folder and run <code>python flip.py -h</code> for further information and usage instructions.</p>
<div class="markdown-heading" dir="auto"><h1 class="heading-element" dir="auto">C++ and CUDA (API and Tool)</h1><a id="user-content-c-and-cuda-api-and-tool" class="anchor" aria-label="Permalink: C++ and CUDA (API and Tool)" href="#c-and-cuda-api-and-tool"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><strong>Setup:</strong></p>
<p dir="auto">The <code>FLIP.sln</code> solution contains one CUDA backend project and one pure C++ backend project.</p>
<p dir="auto">Compiling the CUDA project requires a CUDA compatible GPU. Instruction on how to install CUDA can be found <a href="https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html" rel="nofollow">here</a>.</p>
<p dir="auto">Alternatively, a CMake build can be done by creating a build directory and invoking CMake on the source dir:</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="mkdir build
cd build
cmake ..
cmake --build ."><pre class="notranslate"><code>mkdir build
cd build
cmake ..
cmake --build .
</code></pre></div>
<p dir="auto">CUDA support is enabled via the <code>FLIP_ENABLE_CUDA</code>, which can be passed to CMake on the command line with
<code>-DFLIP_ENABLE_CUDA=ON</code> or set interactively with <code>ccmake</code> or <code>cmake-gui</code>.
<code>FLIP_LIBRARY</code> option allows to output a library rather than an executable.</p>
<p dir="auto"><strong>Usage:</strong></p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="flip[-cuda].exe --reference reference.{exr|png} --test test.{exr|png} [options]"><pre class="notranslate"><code>flip[-cuda].exe --reference reference.{exr|png} --test test.{exr|png} [options]
</code></pre></div>
<p dir="auto">See the <a href="cpp/README.md">README</a> in the <code>cpp</code> folder and run <code>flip[-cuda].exe -h</code> for further information and usage instructions.</p>
<div class="markdown-heading" dir="auto"><h1 class="heading-element" dir="auto">PyTorch (Loss Function)</h1><a id="user-content-pytorch-loss-function" class="anchor" aria-label="Permalink: PyTorch (Loss Function)" href="#pytorch-loss-function"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><strong>Setup</strong> (with Anaconda3):</p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto" data-snippet-clipboard-copy-content="conda create -n flip_dl python numpy matplotlib
conda activate flip_dl
conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c conda-forge
conda install -c conda-forge openexr-python"><pre class="notranslate"><code>conda create -n flip_dl python numpy matplotlib
conda activate flip_dl
conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c conda-forge
conda install -c conda-forge openexr-python
</code></pre></div>
<p dir="auto"><strong>Usage:</strong></p>
<p dir="auto"><em>Remember to activate the</em> <code>flip_dl</code> <em>environment through</em> <code>conda activate flip_dl</code> <em>before using the loss function.</em></p>
<p dir="auto">LDR- and HDR-ꟻLIP are implemented as loss modules in <code>flip_loss.py</code>. An example where the loss function is used to train a simple autoencoder is provided in <code>train.py</code>.</p>
<p dir="auto">See the <a href="pytorch/README.md">README</a> in the <code>pytorch</code> folder for further information and usage instructions.</p>
<div class="markdown-heading" dir="auto"><h1 class="heading-element" dir="auto">Citation</h1><a id="user-content-citation" class="anchor" aria-label="Permalink: Citation" href="#citation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">If your work uses the ꟻLIP tool to find the errors between <em>low dynamic range</em> images,
please cite the LDR-ꟻLIP paper:<br>
<a href="https://research.nvidia.com/publication/2020-07_FLIP" rel="nofollow">Paper</a> | <a href="misc/LDRFLIP.txt">BibTeX</a></p>
<p dir="auto">If it uses the ꟻLIP tool to find the errors between <em>high dynamic range</em> images,
instead cite the HDR-ꟻLIP paper:<br>
<a href="https://research.nvidia.com/publication/2021-05_HDR-FLIP" rel="nofollow">Paper</a> | <a href="misc/HDRFLIP.txt">BibTeX</a></p>
<p dir="auto">Should your work use the ꟻLIP tool in a more general fashion, please cite the Ray Tracing Gems II article:<br>
<a href="https://link.springer.com/chapter/10.1007%2F978-1-4842-7185-8_19" rel="nofollow">Chapter</a> | <a href="misc/FLIP.txt">BibTeX</a></p>
<div class="markdown-heading" dir="auto"><h1 class="heading-element" dir="auto">Acknowledgements</h1><a id="user-content-acknowledgements" class="anchor" aria-label="Permalink: Acknowledgements" href="#acknowledgements"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">We appreciate the following peoples' contributions to this repository:
Jonathan Granskog, Jacob Munkberg, Jon Hasselgren, Jefferson Amstutz, Alan Wolfe, Killian Herveau, Vinh Truong, Philippe Dagobert, Hannes Hergeth, and Matt Pharr.</p>
</article></div>